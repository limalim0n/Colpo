{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa32b057-583a-4ba6-9227-c81e7cc3e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07bf7012-a4d1-486a-8027-f445528f146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d94e2d0-996b-4282-bb7c-7a90d024a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = 'C:\\\\Users\\\\gguid\\\\Desktop\\\\Colpo\\\\ColpoData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0657adf5-8ec2-4c40-aab6-1e52066fe699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the valid class names\n",
    "valid_classes = ['0 Normal', '1 Ectropion', '2 Metaplasia', '3 Bajo grado', '4 Alto grado']\n",
    "\n",
    "# Filter out non-image files and folders and exclude specified folders\n",
    "filtered_classes = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d)) and d in valid_classes and not d.startswith('.')]\n",
    "\n",
    "# Define dataset and apply transformations\n",
    "classified_dataset = ImageFolder(root=root_dir, transform=transform, is_valid_file=lambda f: f.endswith(('.jpg', '.jpeg', '.png', '.ppm', '.bmp')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc510792-467c-4b13-93a0-83e90fe8929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split already classified data into training and validation sets\n",
    "train_data, val_data = train_test_split(classified_dataset, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b80cd-1787-49a4-b2b6-8b5114fa5bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a411eca1-85f8-452a-a3d9-9876e6eea574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "# Define the model\n",
    "#model = torchvision.models.resnet18(pretrained=True)\n",
    "#num_ftrs = model.fc.in_features\n",
    "#model.fc = nn.Linear(num_ftrs, 5)  # 3 output classes\n",
    "\n",
    "# Define the model with updated parameters\n",
    "model = torchvision.models.resnet18(weights=\"ResNet18_Weights.DEFAULT\")\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 5)  # Assuming you have 5 output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b8d1f1e-8a79-4ddc-a53b-4c7d59fae6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b08fb4f-2a44-4f06-bc2f-5d5f72782ca7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.5415\n",
      "Epoch 2/20, Loss: 1.1352\n",
      "Epoch 3/20, Loss: 0.6944\n",
      "Epoch 4/20, Loss: 0.5536\n",
      "Epoch 5/20, Loss: 0.5389\n",
      "Epoch 6/20, Loss: 0.4412\n",
      "Epoch 7/20, Loss: 0.4904\n",
      "Epoch 8/20, Loss: 0.3695\n",
      "Epoch 9/20, Loss: 0.2676\n",
      "Epoch 10/20, Loss: 0.1887\n",
      "Epoch 11/20, Loss: 0.1621\n",
      "Epoch 12/20, Loss: 0.2512\n",
      "Epoch 13/20, Loss: 0.1544\n",
      "Epoch 14/20, Loss: 0.0726\n",
      "Epoch 15/20, Loss: 0.0574\n",
      "Epoch 16/20, Loss: 0.1098\n",
      "Epoch 17/20, Loss: 0.1620\n",
      "Epoch 18/20, Loss: 0.1672\n",
      "Epoch 19/20, Loss: 0.1414\n",
      "Epoch 20/20, Loss: 0.0847\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_data)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'image_classifier.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d613af-d2fc-4493-b792-fc4210d9c9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef36720-7ed4-491f-9411-458fb53f4738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc869599-ac87-4ce8-9e8d-5d42985c5106",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gguid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gguid\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: 1.1.p.0.jpg, Classification: 0 Normal\n",
      "Image: 2.1.p.0.jpg, Classification: 4 Alto grado\n",
      "Image: 2.2.p.0.jpg, Classification: 4 Alto grado\n",
      "Image: 3.1.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 3.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 3.2.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 4.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 4.2.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 5.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 5.2.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 5.3.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 6.1.p.ca.jpg, Classification: 4 Alto grado\n",
      "Image: 7.1.n.metap.jpg, Classification: 0 Normal\n",
      "Image: 7.2.n.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 8.1.n.0.jpg, Classification: 3 Bajo grado\n",
      "Image: 9.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 99.1.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 99.2.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 991.1.p.ca.jpg, Classification: 4 Alto grado\n",
      "Image: 992.1.n.metap.jpg, Classification: 2 Metaplasia\n",
      "Image: 993.1.n.0.jpg, Classification: 2 Metaplasia\n",
      "Image: 994.1.n.metap.jpg, Classification: 2 Metaplasia\n",
      "Image: 995.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 995.2.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 996.1.n.metap.jpg, Classification: 2 Metaplasia\n",
      "Image: 997.1.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 997.2.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 997.3.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 998.1.n.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 999.1.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 9991.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 9991.2.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 9991.3.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 9991.4.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 9992.1.p.l1.jpg, Classification: 4 Alto grado\n",
      "Image: 9993.1.p.0.jpg, Classification: 0 Normal\n",
      "Image: 9993.2.p.0.jpg, Classification: 0 Normal\n",
      "Image: 9994.1.n.ectoprion.jpg, Classification: 2 Metaplasia\n",
      "Image: 9995.1.p.ectoprion.jpg, Classification: 4 Alto grado\n",
      "Image: 9996.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 9997.1.n.ectoprion.jpg, Classification: 0 Normal\n",
      "Image: 9997.2.n.ectoprion.jpg, Classification: 1 Ectropion\n",
      "Image: 9998.1.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 9999.1.n.0.jpg, Classification: 0 Normal\n",
      "Image: 99991.1.p.ca.jpg, Classification: 4 Alto grado\n",
      "Image: 99991.2.p.ca.jpg, Classification: 4 Alto grado\n",
      "Image: 99992.1.n.l1.jpg, Classification: 2 Metaplasia\n",
      "Image: 99993.1.p.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 99993.2.p.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 99993.3.p.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 99994.1.n.0.jpg, Classification: 0 Normal\n",
      "Image: 99995.1.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 99996.1.p.0.jpg, Classification: 4 Alto grado\n",
      "Image: 99996.2.p.0.jpg, Classification: 4 Alto grado\n",
      "Image: 99996.3.p.0.jpg, Classification: 4 Alto grado\n",
      "Image: 99997.1.p.ca.jpg, Classification: 4 Alto grado\n",
      "Image: 99998.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 99998.2.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 99999.1.p.0.jpg, Classification: 0 Normal\n",
      "Image: 99999.2.p.0.jpg, Classification: 4 Alto grado\n",
      "Image: 999991.1.p.l1.jpg, Classification: 4 Alto grado\n",
      "Image: 999992.1.p.metap.jpg, Classification: 0 Normal\n",
      "Image: 999992.2.p.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 999993.1.n.ectoprion.jpg, Classification: 2 Metaplasia\n",
      "Image: 999993.1.p.metap.jpg, Classification: 0 Normal\n",
      "Image: 999993.2.p.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 999994.2.n.0.jpg, Classification: 4 Alto grado\n",
      "Image: 999995.1.p.l1.jpg, Classification: 4 Alto grado\n",
      "Image: 999995.2.p.l1.jpg, Classification: 4 Alto grado\n",
      "Image: 999996.1.n.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 999997.1.pca.jpg, Classification: 4 Alto grado\n",
      "Image: 999998.1.p.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 999998.2.p.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 999998.3.p.metap.jpg, Classification: 4 Alto grado\n",
      "Image: 999999.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 9999991.1.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 9999991.2.p.h2-3.jpg, Classification: 4 Alto grado\n",
      "Image: 9999991.3.p.h2-3.jpg, Classification: 4 Alto grado\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define transformations for the test images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the trained model\n",
    "model = torchvision.models.resnet18(pretrained=False)  # Load the same architecture as trained\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 5)  # Assuming 5 output classes\n",
    "model.load_state_dict(torch.load('image_classifier.pth'))  # Load the trained weights\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define the directory containing the test images\n",
    "quiz_dir = 'C:\\\\Users\\\\gguid\\\\Desktop\\\\Colpo\\\\quiz'\n",
    "\n",
    "# Iterate through each image in the directory\n",
    "for filename in os.listdir(quiz_dir):\n",
    "    if filename.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "        # Load and preprocess the image\n",
    "        image_path = os.path.join(quiz_dir, filename)\n",
    "        image = Image.open(image_path)\n",
    "        image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "        # Classify the image\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "\n",
    "        # Get the class label\n",
    "        class_label = predicted.item()\n",
    "\n",
    "        # Map class index to class name\n",
    "        class_mapping = {0: '0 Normal', 1: '1 Ectropion', 2: '2 Metaplasia', 3: '3 Bajo grado', 4: '4 Alto grado'}\n",
    "        predicted_class = class_mapping[class_label]\n",
    "\n",
    "        # Print the name of the image and its classification\n",
    "        print(f\"Image: {filename}, Classification: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5326f3-b7ba-465a-ace5-e96ebb105f08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c146f0ec-59a8-4821-81f3-203d2e92ba37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1adfa6-c5fb-4280-be1e-72b722506e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d67cf-54a2-4d83-8e5d-cb6016c6474e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe79f7ea-c4fa-402c-ac6d-deba6bc787bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([5, 512]) from checkpoint, the shape in current model is torch.Size([3, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([3]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m num_ftrs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_ftrs, \u001b[38;5;241m3\u001b[39m)  \u001b[38;5;66;03m# Assuming 3 output classes\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_classifier.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Load the trained weights\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Define transformations for the new image\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet:\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([5, 512]) from checkpoint, the shape in current model is torch.Size([3, 512]).\n\tsize mismatch for fc.bias: copying a param with shape torch.Size([5]) from checkpoint, the shape in current model is torch.Size([3])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load the trained model\n",
    "model = torchvision.models.resnet18(pretrained=False)  # Load the same architecture as trained\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)  # Assuming 3 output classes\n",
    "model.load_state_dict(torch.load('image_classifier.pth'))  # Load the trained weights\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Define transformations for the new image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load and preprocess the new image\n",
    "image_path = 'C:\\\\Users\\\\gguid\\\\Desktop\\\\Colpo\\\\ColpoData\\\\0 Normal\\\\4.jpg'\n",
    "image = Image.open(image_path)\n",
    "image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Classify the image\n",
    "with torch.no_grad():\n",
    "    output = model(image_tensor)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "\n",
    "# Get the class label\n",
    "class_label = predicted.item()\n",
    "\n",
    "# Print the predicted class\n",
    "class_mapping = {0: 'Normal', 1: 'Ectropion', 2: 'Metaplasia'}\n",
    "predicted_class = class_mapping[class_label]\n",
    "print(\"Predicted class:\", predicted_class)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
